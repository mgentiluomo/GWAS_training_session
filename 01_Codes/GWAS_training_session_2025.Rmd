---
title: "GWAS training session"
output: html_notebook
authors:
  - name: "Dr. Manuel Gentiluomo"
    affiliation: "University of Pisa"
    url: "https://github.com/mgentiluomo"
    email: "manuel.gentiluomo@unipi.it"
  - name: "Riccardo Farinella"
    affiliation: "University of Pisa"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
---

üß¨ Array Data Preparation: From Raw Genotypes to PLINK FormatBefore commencing any GWAS analysis, it's crucial to understand the origin and structure of the genetic data. This data is typically generated by Genotyping Array technologies (such as Illumina or Affymetrix) which detect genetic variants (the genotype) at thousands or millions of known genomic locations (SNPs).

1. üî† How Array Data (Raw Data) is Generated and Composed
A Genotyping Array is a chip that allows us to simultaneously test many Single Nucleotide Polymorphisms (SNPs).
Generation:
DNA from each individual is hybridised to the chip. The array software (e.g., Illumina's GenomeStudio) reads the signal intensity for the two possible alleles (genetic variants, e.g., A vs. G) at each SNP position.
Composition (Raw Genotype Data):
The result isn't immediately in a PLINK-readable format, but is usually a text file containing: 
- Sample Identifier (Sample ID): The individual being analysed.
- SNP Identifier (SNP ID): The tested genetic location.
- Raw Allelic Genotype: The raw genotype reading for that sample and SNP (e.g., AA, AG, GG, or sometimes in A/T, C/G format, etc.).

https://unipiit-my.sharepoint.com/:f:/g/personal/a035503_unipi_it/EvcItWiDOsRAn38jZlsxlZQBfta4Ifl0Y-W88ngGI7CZZQ?e=73j4pV

2. ‚û°Ô∏è Converting to PLINK FormatPLINK software is the de facto standard tool for GWAS analysis, but it requires data to be in a specific format to operate efficiently. This format is known as the PLINK Binary format (.bed, .bim, .fam). 
Extension | File | NameContent
.fam | Pedigree/Phenotype | Information about individuals (Family ID, Individual ID, Sex, Phenotype Status).
.bim | Genetic Map | Information about the SNPs (Chromosome, Physical Position, Alleles).
.bed | Genotype Data| The actual binary genotypes (AA, AG, GG, etc.) compressed into an efficient format for PLINK reading.

The Step from Array Matrix to PLINK:
1) Cleaning and Re-formatting: The raw output file must first be cleaned and converted into an intermediate text format (sometimes called PLINK Long Format or TPED/TFAM format).
2) PLINK Conversion: The command plink --make-bed is then used on the text format to generate the three essential binary files (.bed, .bim, and .fam).

Our entire training session will commence using files already in the PLINK Binary format, which you will find located in the data/folder.

DATA FOLDER
https://unipiit-my.sharepoint.com/:f:/g/personal/a035503_unipi_it/EsmOBic2ICxIgMInykR6CfQBaz8XbmstRE13VUo0KFD7qQ?e=m1b5ln




############################################################ #############################
üõ†Ô∏è Environment Setup and Prerequisites
Before starting the analysis pipeline, please follow these steps to organize your files and ensure you have all the necessary software installed. This organization is critical for the reproducibility of the entire course.

1. Folder Setup
Create the Main Project Folder: On your Desktop, create a new folder named Class_folder.

Save the Notebook: Place this R Markdown Notebook (.Rmd file) directly inside the Class_folder/.

Data Placement: All subsequent data files (PLINK data, phenotype files, etc.) that you download will be placed inside the Class_folder/ as well, usually in the 01_Database/ directory, which will be created automatically.

Your final folder structure should look like this initially:

Desktop/
‚îî‚îÄ‚îÄ Class_folder/
    ‚îú‚îÄ‚îÄ Your_GWAS_Notebook.Rmd (This file)
    ‚îî‚îÄ‚îÄ (Other files will be created or placed here)

2. Required Software
You must have the following core software installed and accessible on your system:

R and RStudio: You should have the latest versions of R and the RStudio IDE installed. RStudio is required to run the R Markdown Notebook effectively.

PLINK: The entire Quality Control (QC) and Association Analysis pipeline relies heavily on the command-line genetic analysis tool, PLINK.

Action: You need to download the appropriate executable file for your operating system (Windows, macOS, or Linux) from the official PLINK website.

Accessibility: Ensure that the PLINK executable is either saved directly inside your Class_folder/ or, ideally, added to your system's PATH so that the system("plink ...") commands in this notebook can execute it from anywhere.

(PLINK installation and usage details will be covered in the next sections or in your accompanying slides.)

############################################################ #############################


üíª R Code Explanation: Package Installation and Setup
This specific code chunk is crucial for ensuring a smooth start to your GWAS training session. Its primary goal is to check, install, and load all the necessary R packages required for the downstream data analysis, manipulation, and visualisation.

```{r #1.a - Installation of needed packages}
# Installation of packages
packages <- c("base", "readxl", "readr", "dplyr", "ggplot2", "rgl", "car", "utils", "qqman", "GGally", "ggfortify")
for (pkg in packages) {
  if (!require(pkg, character.only = TRUE)) {
    install.packages(pkg)
    library(pkg, character.only = TRUE)
  }
}
rm(packages,pkg)
```

A fundamental step in reproducible analysis is ensuring your script knows where to find the input data (data/ folder) and where to save the results (results/ folder).

When working with R Markdown, the working directory often defaults to the location of the .Rmd file itself. The following code automatically detects the path of the currently running notebook, regardless of whether you are executing it interactively in RStudio or rendering it to HTML/PDF.


Move this notebook inside the working folder
in our case is Class_folder/

```{r #1.b1 - Set of working directories}
#This chunk uses specialized functions to determine the exact folder where this notebook file resides.

# Check if the script is running interactively or being rendered
if (interactive()) {
  # Get the directory of the current R Markdown file in RStudio
  library(rstudioapi)
  notebook_dir <- dirname(rstudioapi::getActiveDocumentContext()$path)
} else {
  # Get the directory of the file during rendering
  notebook_dir <- dirname(normalizePath(knitr::current_input()))
}

# Print the directory for verification
print(notebook_dir)

```

This code chunk sets the local working directory and then defines a structured directory hierarchy to organise all intermediate files, data, and results generated throughout the multi-stage GWAS pipeline. This organisation is crucial for reproducibility and easy navigation.

Code Explanation: Defining Paths and Creating Folders
I. Establishing the Main Root
II. Defining the Pipeline Subdirectories
III. Creating the Folders

```{r #1.b2 - üõ†Ô∏è 1.b.2 - Setting up the GWAS Directory Structure}
# Creation of working directories
# Set the working directory to the notebook's directory
setwd(notebook_dir)

# Define the main directory as the current working directory
main <- paste0(getwd(), "/")

#The script first uses setwd(notebook_dir) to set the R working directory to the location detected in the previous chunk (1.b.1). 
#main is then defined as the absolute path to the notebook's directory. This serves as the root from which all subsequent folders will branch.

# Define subdirectory paths for various stages of the workflow
DB_dir <- paste0(main, "01_Database/")               # Directory for database files
QC_dir <- paste0(main, "02_QC/")                    # Directory for quality control files
PCA_dir <- paste0(main, "03_PCA/")                  # Directory for principal component analysis results
Imputation_dir <- paste0(main, "04_Imputation/")    # Directory for imputed data
PostImputation_dir <- paste0(main, "05_PostImputation/") # Directory for post-imputation quality control
Analysis_dir <- paste0(main, "06_Statistical_analysis/") # Directory for statistical analysis results
Other <- paste0(main, "Other/")                    # Directory for miscellaneous files

# Combine all directory paths into a list for easier handling
dirs <- c(DB_dir, QC_dir, PCA_dir, Imputation_dir, PostImputation_dir, Analysis_dir, Other)

# Create the directories if they don't exist
for (dir in dirs) {
  if (!dir.exists(dir)) {                          # Check if the directory already exists
    dir.create(dir, recursive = TRUE)             # Create the directory (and parent directories, if needed)
  }
}

# Print a confirmation message
text <- paste("End of chunk #1.b - Creation of working directories", "Your MAIN working directory is:", main, "PLEASE verify!",sep = "\n") 
cat(text)        
# End of chunk 1

```

 #2 Plink
Download Links for PLINK 2.0

Link to PLINK2 website https://www.cog-genomics.org/plink/2.0/

You can download PLINK 2.0 for various operating systems using the links below:

Build Alpha 6.1 (14 Nov 2024)
#### Linux
# - [PLINK 2.0 for Linux (AVX2 Intel)](https://s3.amazonaws.com/plink2-assets/alpha6/plink2_linux_avx2_20241114.zip)
# - [PLINK 2.0 for Linux (AVX2 AMD)](https://s3.amazonaws.com/plink2-assets/alpha6/plink2_linux_amd_avx2_20241114.zip)
# - [PLINK 2.0 for Linux (64-bit Intel)](https://s3.amazonaws.com/plink2-assets/alpha6/plink2_linux_x86_64_20241114.zip)
# - [PLINK 2.0 for Linux (32-bit)](https://s3.amazonaws.com/plink2-assets/alpha6/plink2_linux_i686_20241114.zip)

#### macOS
# - [PLINK 2.0 for macOS (M1)](https://s3.amazonaws.com/plink2-assets/alpha6/plink2_mac_arm64_20241114.zip)
# - [PLINK 2.0 for macOS (AVX2)](https://s3.amazonaws.com/plink2-assets/alpha6/plink2_mac_avx2_20241114.zip)
# - [PLINK 2.0 for macOS (64-bit)](https://s3.amazonaws.com/plink2-assets/alpha6/plink2_mac_20241114.zip)

#### Windows
# - [PLINK 2.0 for Windows (AVX2)](https://s3.amazonaws.com/plink2-assets/alpha6/plink2_win_avx2_20241114.zip)
# - [PLINK 2.0 for Windows (64-bit)](https://s3.amazonaws.com/plink2-assets/alpha6/plink2_win64_20241114.zip)
# - [PLINK 2.0 for Windows (32-bit)](https://s3.amazonaws.com/plink2-assets/alpha6/plink2_win32_20241114.zip)

# For more details and additional versions, visit the [PLINK 2.0 Homepage](https://www.cog-genomics.org/plink/2.0/).

---

### Instructions
1. Click on the appropriate link for your operating system.
2. The download will start immediately.
3. Extract the downloaded ZIP file and follow the instructions in the [PLINK 2.0 Documentation](https://www.cog-genomics.org/plink/2.0/).



```{r #2 Plink}
# Plink testing

# Ensure the PLINK executable is in the main directory
# Before running the following commands, copy and paste the PLINK executable (e.g., plink.exe) 
# into the main directory. This ensures the system can locate and execute PLINK commands.

# Verify the functionality of Plink
# This command checks the installed version of Plink
system("plink --version")

# This command displays the list of available functions in Plink
system("plink --help")

# Print a confirmation message for the end of this code chunk
print(system("plink --version"))
print("End of chunk 2")
# End of chunk 2

```


The first essential step in Sample QC is checking for inconsistencies between the reported sex (as listed in the .fam file or provided as an external phenotype) and the genetically inferred sex (derived from X chromosome heterozygosity). Discrepancies can bias subsequent calculations like HWE checks.
Code Explanation: The 7-Step Sex Check Protocol
This block automates a rigorous process using both PLINK (for genetic calculations and filtering) and R (for viewing and selecting problematic samples).

Step	
R/PLINK Action	
Purpose

1: Update Sex Data	
system("plink --bfile ... --update-sex ...")
Synchronises the sex information in the current .fam file (DB_A_updated) with an external file (Sex.txt). This ensures we start with the most current reported data. A new binary file (QC1_sex) is created.
---------
2: Verify Consistency
system("plink ... --check-sex ...")
PLINK calculates the genetically inferred sex based on X chromosome data. It generates a report (results_checksex.sexcheck) listing the expected sex, the inferred sex, and a STATUS field (OK or PROBLEM).
--------
3-4: Identify Problems
read_table(...), filter(...), write.table(...)
R takes over: The .sexcheck file is loaded. We use dplyr::filter to isolate only individuals where the STATUS is "PROBLEM" (i.e., male reported but genetically female, or vice-versa). The IDs of these individuals are saved to a clean file (sex-discrepancy.txt).
--------
5: Remove Problematic Samples
system("plink ... --remove ...")
PLINK removes the identified samples listed in sex-discrepancy.txt. This creates the next clean dataset (QC2_sex)
--------
6: Investigate Discrepancies (Optional)
read.delim(...), merge(...)
This step is for diagnostic purposes. It loads the .hh (homozygosity haploid) output from PLINK and merges it with the .bim file to analyze the genomic location of the variants that caused the sex discrepancy flag. This helps ensure the issue is biological and not a systematic array problem.
--------
7: Clean up Sex Discrepancies
system("plink ... --set-hh-missing ...")
This is a final cleaning step: it sets the genotypes of the variants that were inconsistent with the inferred sex to missing (NA). This prevents these specific variants from causing further issues in downstream analysis, resulting in the final file for this step (QC2_sex_cleaned).

```{r #3.a - Quality Control (QC) Check for sex discrepancies}
# SECTION 1: Check for sex discrepancies

# Step 1: Update sex data
# This updates the sex information based on the file "Sex.txt" 
# and creates a new binary file with the updated data.
system("plink --bfile 01_Database/DB_A_updated --update-sex 01_Database/Sex.txt --make-bed --out 02_QC/QC1_sex")

# Step 2: Verify consistency between reported and genetic sex
# This generates a report on discrepancies between reported and genetic sex.
system("plink --bfile 02_QC/QC1_sex --check-sex --out 02_QC/results_checksex")

# Step 3: Review the results
# Load and visualise the results from the sex check.
results_checksex <- read_table(paste(QC_dir, "results_checksex.sexcheck", sep = ""))
View(results_checksex)

# Step 4: Identify individuals with sex inconsistencies
# Filter out individuals marked as "PROBLEM" in the results.
results_checksex <- filter(results_checksex, STATUS == "PROBLEM") %>%
  select(FID, IID)

# Save the list of problematic individuals to a file.
write.table(as.data.frame(results_checksex), paste(QC_dir, "sex-discrepancy.txt", sep = ""), sep = "\t", quote = FALSE, row.names = FALSE, col.names = FALSE)

# Step 5: Remove individuals with sex inconsistencies
# Exclude the individuals identified in the previous step.
system("plink --bfile 02_QC/QC1_sex --remove 02_QC/sex-discrepancy.txt --make-bed --out 02_QC/QC2_sex")

# Step 6: Investigate sex discrepancies
# Load the variant data related to sex discrepancies for detailed analysis.
QC2_sex_variants <- read.delim(paste(main, "02_QC/QC2_sex.hh", sep = ""), header = FALSE)
BIM <- read.delim(paste(main, "02_QC/QC2_sex.bim", sep = ""), header = FALSE)

# Assign column names for better understanding.
names(BIM) <- c("Chr", "SNP", "cM", "Pos", "A1", "A2")
names(QC2_sex_variants) <- c("FID", "IID", "SNP")

# Merge variant data with BIM file for additional information.
QC2_sex_variants <- merge(QC2_sex_variants, BIM, by = "SNP")

# Summary of chromosome distribution for problematic variants.
summary(as.factor(QC2_sex_variants$Chr))

# Step 7: Clean up sex discrepancies
# Set problematic variants to missing.
system("plink --bfile 02_QC/QC2_sex --set-hh-missing --make-bed --out 02_QC/QC2_sex_cleaned")
```

This completed the sex check and resulted in a clean PLINK dataset (QC2_sex_cleaned). What is the next QC step in your pipeline?
Raw array data sometimes includes markers that are not located on the main human chromosomes, such as mitochondrial DNA (MT), pseudo-autosomal regions (PARs) that need special handling, or unmapped markers (coded as 0 in the .bim file). These non-standard markers can interfere with downstream analyses, particularly PCA and imputation.

This step ensures we retain only the genetic data relevant for standard association testing by filtering to chromosomes 1 through 23 (where 23 typically represents the X chromosome, depending on the PLINK version and input data conventions).

The following command uses a simple PLINK flag to keep only markers assigned to chromosomes 1 through 23.
#Command
PLINK Action
@Purpose
--
# plink --bfile ...
Loads the cleaned data from the sex check (QC2_sex_cleaned).
@Sets the input dataset.
--------
# --chr 1-23
Filters the data to retain only markers (SNPs) whose chromosome column in the .bim file is between 1 and 23, inclusive.
@Excludes mitochondrial and unplaced markers.
--------
# --make-bed --out ...
Creates the new binary files: QC3_chr_selection.bed, .bim, and .fam.
@Saves the filtered dataset for the next QC step.

```{r #3.b - Quality Control (QC) Chromosome selection}
# SECTION 2: Chromosome selection

# Retain only autosomal and sex chromosomes (1-23).
system("plink --bfile 02_QC/QC2_sex_cleaned --chr 1-23 --make-bed --out 02_QC/QC3_chr_selection")
```


3.c - Quality Control (QC): Minor Allele Frequency (MAF) Filtering üìâ
The Minor Allele Frequency (MAF) is the frequency of the least common allele at a specific SNP position within your study population.
Variants with a very low MAF (e.g., less than 1%) are generally excluded from standard GWAS because:
- Low Statistical Power: It is extremely difficult to achieve statistical significance for associations involving very rare variants, especially with smaller sample sizes.
- Genotyping Errors: Low MAF variants are more susceptible to being confused with random genotyping errors.

This step sets a standard threshold of $1\%$ ($0.01$).

```{r #3.c - Quality Control (QC) Minor Allele Frequency (MAF) filtering}
# SECTION 3: Minor Allele Frequency (MAF) filtering

# Exclude SNPs with a minor allele frequency (MAF) below 1%.
system("plink --bfile 02_QC/QC3_chr_selection --maf 0.01 --write-snplist --make-bed --out 02_QC/QC4_maf")
```


3.d - Quality Control (QC): Missing Data and Call Rate Filtering üìä
The Call Rate refers to the percentage of non-missing genotype calls available for a given SNP or sample. High missingness rates indicate unreliable data and must be filtered out. We apply two distinct filters: one for SNPs (--geno) and one for samples (--mind).

I. SNP Missingness (--geno)
The --geno filter removes markers (SNPs) where a high proportion of individuals have missing genotype data.

Rationale: A SNP with a high missing rate is often associated with a failed probe on the array or a complex genomic region that is difficult to genotype accurately. Retaining such SNPs introduces noise into the analysis.

II. Sample Missingness (--mind)
The --mind filter removes individuals (samples) that have missing genotype data across a high proportion of all the tested markers.

Rationale: An individual with a high missing rate usually indicates poor quality or low quantity of the starting DNA material. Retaining such individuals can disproportionately affect the accuracy of allele frequency and association estimates.

```{r #3.d - Quality Control (QC) Call rate filtering}
# SECTION 4: Call rate filtering

# Exclude SNPs with missing genotype data above 5%.
system("plink --bfile 02_QC/QC4_maf --geno 0.05 --write-snplist --make-bed --out 02_QC/QC5_geno")

# Exclude individuals with missing genotype data above 10%.
system("plink --bfile 02_QC/QC5_geno --mind 0.10 --make-bed --out 02_QC/QC6_mind")
```


3.e - Quality Control (QC): Heterozygosity Check üß¨
The Heterozygosity Rate measures the proportion of genetic markers at which an individual possesses two different alleles (i.e., is heterozygous). An individual's observed heterozygosity should fall within a normal range relative to the study population.
Rationale for Filtering
Samples showing extreme heterozygosity rates are usually indicative of problems:
- Excessive Heterozygosity (Outlier High): Often suggests sample contamination (DNA from two individuals mixed) or, less commonly, unknown cryptic relatedness.
- Low Heterozygosity (Outlier Low): Can indicate inbreeding or a hidden technical issue, such as contamination with non-human DNA.

We use a standard threshold to identify these outliers: 3 Standard Deviations (SD) away from the population mean.

```{r #3.e - Quality Control (QC) Heterozygosity check}
# SECTION 5: Heterozygosity check

# Calculate heterozygosity rates for each individual.
system("plink --bfile 02_QC/QC6_mind --het --out 02_QC/heterozygosity")

# Load heterozygosity data and identify outliers (¬±3 SD from the mean).
dat <- read.table(paste(QC_dir, "heterozygosity.het", sep = ""), header = TRUE)
m <- mean(dat$F)
s <- sd(dat$F)
valid <- subset(dat, F <= m + 3 * s & F >= m - 3 * s)

# Save the list of valid individuals.
write.table(valid[, c("FID", "IID")], paste(QC_dir, "DB_valid_sample.txt", sep = ""), quote = FALSE, row.names = FALSE)

# Retain only individuals with acceptable heterozygosity.
system("plink --bfile 02_QC/QC6_mind --keep 02_QC/DB_valid_sample.txt --make-bed --out 02_QC/QC7_het")
```


3.f - Quality Control (QC): Cryptic Relatedness Check üîó
Cryptic Relatedness refers to family ties (e.g., first or second-degree relatives) within a study cohort that were not known or reported initially. GWAS association testing assumes independence among samples. To prevent inflated results, related individuals must be identified and removed, or accounted for in the statistical model.

We use a two-part process: first, creating a set of independent markers, and second, calculating the genetic relationship between all pairs of individuals.

```{r #3.f - Quality Control (QC) Cryptic relatedness check}
# SECTION 6: Cryptic relatedness check

# Step 1: Prune SNPs for relatedness analysis.
system("plink --bfile 02_QC/QC7_het --indep-pairwise 200 50 0.25 --out 02_QC/Pruned_list")

# Step 2: Identify and exclude related individuals (threshold: 0.2).
system("plink --bfile 02_QC/QC7_het --extract 02_QC/Pruned_list.prune.in --rel-cutoff 0.2 --out 02_QC/subject_rel")

# Step 3: Create a dataset excluding related individuals.
system("plink --bfile 02_QC/QC7_het --keep 2_QC/subject_rel.rel.id --make-bed --out 02_QC/QC8_rel")
```

3.g - Quality Control (QC): Hardy-Weinberg Equilibrium (HWE) Filtering ‚öñÔ∏è
The Hardy-Weinberg Equilibrium (HWE) principle states that allele and genotype frequencies in a large, randomly mating population remain constant from generation to generation, provided no other evolutionary forces (like mutation, selection, or migration) are at play.
Rationale for Filtering
In a GWAS context, deviations from HWE (a very small p-value) are often not due to natural evolutionary forces, but rather indicate technical problems:
- Genotyping Error: The most common cause; the assay may be consistently failing to call specific genotypes accurately.
-Population Substructure: If the study population is a mix of highly divergent groups, HWE can be violated.We use a stringent p-value threshold (typically 1e-5) to remove problematic SNPs.

```{r #3.g - Quality Control (QC) Hardy-Weinberg equilibrium filtering}
# SECTION 7: Hardy-Weinberg equilibrium filtering

# Filter SNPs failing the Hardy-Weinberg equilibrium test (p-value < 1e-5).
system("plink --bfile 02_QC/QC8_rel --hwe 1e-5 --pheno 01_Database/pheno.txt --write-snplist --make-bed --out 02_QC/QC9_hwe")
```

3.h - Save Environment: Backup and Reproducibility üíæ
This final chunk of the Quality Control module is purely for workflow management and safety.

Rationale for Saving the Environment
In a lengthy analysis session that involves multiple steps, it's crucial to periodically save the R environment. The environment contains all the variables you have defined, including:

The detected directory paths (main, QC_dir, etc.).

Any intermediate data tables you loaded (e.g., the results_checksex table from the sex check).

Any objects created during diagnostic checks.

Saving the environment allows you to:

Resume Work: If your R session terminates unexpectedly, you can load the saved file (.RData) and continue exactly where you left off.

Ensure Reproducibility: It provides a snapshot of the exact environment used to produce the QC-filtered dataset (QC9_hwe).


```{r #3.h - Quality Control (QC) Save environment}
# SECTION 8: Save environment

# Save the R environment for backup and reproducibility.
save.image(paste(main, "Env_chunk3.RData", sep = ""))

# Confirm completion of the chunk.
print("End of chunk 3")
# End of chunk 3

```


#############################################################################################################



```{r #4.a DB preparation for PCA computation and identification of ancestry}
# PCA computation and identification of ancestry

# STEP 1: Extraction of independent SNPs
# Extract independent SNPs from the 1000 Genomes database for PCA.
system("plink --bfile 3_PCA/DB_1000G --extract 2_QC/QC9_hwe.bim --make-bed --out 3_PCA/DB_1000G_reduced")

# STEP 2: Merging 1000 Genomes database with our data
# Merge the datasets for PCA computation.
system("plink --bfile 2_QC/QC9_hwe --bmerge 3_PCA/DB_1000G_reduced.bed 3_PCA/DB_1000G_reduced.bim 3_PCA/DB_1000G_reduced.fam --make-bed --out 3_PCA/DB_merge_v1")

# STEP 3: Resolving inconsistencies by flipping SNPs
# Check if the .missnp file exists before flipping SNPs
if (file.exists(paste(PCA_dir, "DB_merge_v1-merge.missnp", sep=""))) {
  system("plink --bfile 2_QC/QC9_hwe --flip 3_PCA/DB_merge_v1-merge.missnp --make-bed --out 2_QC/QC10_flipped")
  
  # STEP 3.a: Merging again after flipping
  system("plink --bfile 2_QC/QC10_flipped --bmerge 3_PCA/DB_1000G_reduced.bed 3_PCA/DB_1000G_reduced.bim 3_PCA/DB_1000G_reduced.fam --make-bed --out 3_PCA/DB_merge_v2")
  
  if (file.exists(paste(PCA_dir, "DB_merge_v2-merge.missnp", sep=""))) {
    # STEP 3.b: Removing unresolved SNPs
    system("plink --bfile 2_QC/QC10_flipped --exclude 3_PCA/DB_merge_v2-merge.missnp --make-bed --out 2_QC/QC11")
  } else {
    # No further inconsistencies detected after the second merge
    message("No further inconsistencies detected after flipping.")
    # Use QC10_flipped as the final file
    system("plink --bfile 2_QC/QC10_flipped --make-bed --out 2_QC/QC11")
  }
} else {
  # No inconsistencies detected in the first merge
  message("No inconsistencies detected. Flipping SNPs is not necessary.")
  # Use QC9_hwe as the final file
  system("plink --bfile 2_QC/QC9_hwe --make-bed --out 2_QC/QC11")
}

# STEP 4: Final merging
system("plink --bfile 2_QC/QC11 --bmerge 3_PCA/DB_1000G_reduced.bed 3_PCA/DB_1000G_reduced.bim 3_PCA/DB_1000G_reduced.fam --make-bed --out 3_PCA/DB_merged")
```

```{r #4.b PCA computation}
# STEP 5: SNP and subject call rate filtering
system("plink --bfile 3_PCA/DB_merged --geno 0.05 --mind 0.10 --make-bed --out 3_PCA/DB_merged_clean")

# STEP 6: Identification of independent SNPs
system("plink --bfile 3_PCA/DB_merged_clean --indep-pairwise 200 50 0.25 --out 3_PCA/SNP_to_PCA")

# STEP 7: Extraction of independent SNPs
system("plink --bfile 3_PCA/DB_merged_clean --extract 3_PCA/SNP_to_PCA.prune.in --make-bed --out 3_PCA/DB_to_PCA")

# STEP 8: PCA calculation
system("plink --bfile 3_PCA/DB_to_PCA --pca var-wts --out 3_PCA/PCA")
```

```{r #4.c PCA computation and ancestry identification}
# PCA computation and ancestry identification

# Define a function to calculate Mahalanobis distance
# This function calculates the Mahalanobis distance of unknown individuals from a given population.
# The Mahalanobis distance considers the covariance of the population data, making it suitable for multivariate comparisons.
calculate_mahalanobis <- function(population_data, unk_data) {
  mean_pop <- colMeans(population_data)  # Calculate mean for each PCA component in the population
  cov_pop <- cov(population_data)       # Calculate the covariance matrix for the population
  if (det(cov_pop) == 0) stop("Covariance matrix is singular, cannot compute Mahalanobis distances.")  # Handle singular matrices
  mahalanobis_distances <- mahalanobis(unk_data[, c("PC1", "PC2")], center = mean_pop, cov = cov_pop)  # Compute distances
  return(mahalanobis_distances)  # Return the calculated distances
}

# Default PCA directory (set a fallback if not defined)
PCA_dir <- ifelse(exists("PCA_dir"), PCA_dir, "./")

# Load PCA eigenvalues (explains variance for each principal component)
PCA_val <- read.table(file.path(PCA_dir, "PCA.eigenval"), quote = "", comment.char = "")
names(PCA_val) <- "Eigenvalue"
PCA_val$PC = paste("PC",1:length(PCA_val$Eigenvalue), sep="")
# Calculate the variance explained by each principal component
PCA_val$ExplainedVariance <- PCA_val$Eigenvalue/sum(PCA_val$Eigenvalue)
# Calculate the cumulative variance obtained by summing each PC contribution
PCA_val$CumulativeVariance <- cumsum(PCA_val$ExplainedVariance)

# Draw a scree plot
# Ensure the PC column is ordered as a factor
PCA_val$PC <- factor(PCA_val$PC, levels = paste("PC", 1:length(PCA_val$Eigenvalue), sep = ""))

# Draw the plot
# Ensure the PC column is ordered as a factor
PCA_val$PC <- factor(PCA_val$PC, levels = paste("PC", 1:length(PCA_val$Eigenvalue), sep = ""))
scree_p = ggplot(PCA_val, aes(x = PC)) +
    # Bar plot for variance explained
    geom_bar(aes(y = ExplainedVariance * 100, fill = "Explained Variance"), 
             stat = "identity", alpha = 0.7) +
    # Line for cumulative variance
    geom_line(aes(y = CumulativeVariance * 100, color = "Cumulative Variance"), 
              group = 1, size = 1) +
    # Points for cumulative variance
    geom_point(aes(y = CumulativeVariance * 100, color = "Cumulative Variance"), 
               size = 2) +
    # Primary y-axis (left): Explained variance in percentage
    scale_y_continuous(
        name = "Explained Variance (%)",
        labels = scales::label_percent(scale = 1), # Left y-axis as percentage
        sec.axis = sec_axis(
            trans = ~ .,  # No transformation, same scale for right axis
            name = "Cumulative Variance (%)",
            labels = scales::label_percent(scale = 1) # Right y-axis as percentage
        )
    ) +
    # Define colors and labels for the legend
    scale_fill_manual(values = c("skyblue")) +
    scale_color_manual(values = c("Cumulative Variance" = "red")) +
    labs(
        title = "Variance Explained and Cumulative Variance",
        x = "Principal Components",
        fill = "",  # Removes the legend title for fill
        color = ""  # Removes the legend title for color
    ) + theme_bw() +  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "top")  # Adjust legend position (optional)
  
# Save the scree plot
ggsave(filename = file.path(PCA_dir, paste0("scree_plot.jpeg")), plot = scree_p, width = 8, height = 6, dpi = 600)

# Load PCA eigenvectors (the principal components for each individual)
PCA <- read.table(file.path(PCA_dir, "PCA.eigenvec"), quote = "", comment.char = "")

# Calculate the number of PCA components (columns beyond FID and IID)
num_pca_columns <- ncol(PCA) - 2

# Rename the columns of PCA data for clarity
colnames(PCA) <- c("FID", "IID", paste0("PC", 1:num_pca_columns))

# Load population data from the 1000 Genomes project
# This dataset contains known population labels for individuals
ID_1KG <- read_excel(file.path(PCA_dir, "1000G_individuals.xlsx"), col_names = FALSE)
colnames(ID_1KG) <- c("FID", "IID", "POP")  # Rename columns for clarity

# Merge the PCA results with the population data
# This step adds known population labels (POP) to the PCA data
dfPCA <- merge(PCA, ID_1KG, by = c("FID", "IID"), all.x = TRUE)
dfPCA$POP[is.na(dfPCA$POP)] <- "unk"  # Replace missing population labels with "unk" (unknown)

# Filter PCA data by population group (EUR, EAS, AFR, and unknown)
# These subsets will be used to compute Mahalanobis distances
populations <- list(
  EUR = dfPCA %>% filter(POP == "EUR") %>% select(PC1, PC2),  # European population
  EAS = dfPCA %>% filter(POP == "EAS") %>% select(PC1, PC2),  # East Asian population
  AFR = dfPCA %>% filter(POP == "AFR") %>% select(PC1, PC2)   # African population
)
# Subset unknown individuals (those without a known population label)
unk_data <- dfPCA %>% filter(POP == "unk") %>% select(FID, IID, PC1, PC2)

# Calculate Mahalanobis distances for unknown individuals relative to each population group
# Loop through each population group and calculate distances
for (pop in names(populations)) {
  unk_data[[paste0("mahalanobis_", pop)]] <- calculate_mahalanobis(populations[[pop]], unk_data)
}

# Assign the closest population to each unknown individual
# This is determined by the smallest Mahalanobis distance
unk_data <- unk_data %>%
  mutate(closest_population = case_when(
    mahalanobis_EUR <= mahalanobis_EAS & mahalanobis_EUR <= mahalanobis_AFR ~ "EUR",  # Closest to European
    mahalanobis_EAS <= mahalanobis_EUR & mahalanobis_EAS <= mahalanobis_AFR ~ "EAS",  # Closest to East Asian
    mahalanobis_AFR <= mahalanobis_EUR & mahalanobis_AFR <= mahalanobis_EAS ~ "AFR"   # Closest to African
  ))

# Save the results of the closest population assignment
write.table(unk_data, file.path(PCA_dir, "closestPOP.txt"), sep = "\t", row.names = FALSE, quote = FALSE)

# Save subsets of unknown individuals for each population group
# This step creates separate files for individuals assigned to EUR, EAS, and AFR
for (pop in names(populations)) {
  subset <- filter(unk_data, closest_population == pop)
  write.table(subset, file.path(PCA_dir, paste0("closestPOP_", pop, ".txt")), sep = "\t", row.names = FALSE, quote = FALSE)
}

# Create a new column in dfPCA with the predicted population
# Replace "unk" with the predicted population for unknown individuals
new_call <- unk_data %>%
  mutate(newPOP = paste("new_", closest_population, sep = "")) %>%
  select(IID, newPOP)

# Merge the new population predictions back into the PCA dataframe
dfPCA <- dfPCA %>%
  left_join(new_call, by = "IID") %>%
  mutate(POP_final = ifelse(POP == "unk", newPOP, POP))  # Use predicted population for "unk"

# Visualize PCA results using ggplot2 and ggfortify
# Perform PCA analysis
explained_var <- as.vector(round(slice(select(PCA_val, c("ExplainedVariance")), c(1,2))*100, digits = 2)$ExplainedVariance)  # Select the variance explained by the first two components
pca_res <- prcomp(select(dfPCA, starts_with("PC")), scale. = TRUE)  # Perform PCA scaling the data

# Generate PCA plots
# Plot 1: Original population labels
plot1 <- autoplot(pca_res, data = dfPCA, colour = "POP") + theme_bw() + xlab(paste("PC1 (",  explained_var[1], "%)", sep="")) + ylab(paste("PC2 (",  explained_var[2], "%)", sep=""))+ ggtitle(paste("PCA by Original POP", sep = ""))

# Plot 2: Assigned population labels
plot2 <- autoplot(pca_res, data = dfPCA, colour = "POP_final") + theme_bw() + xlab(paste("PC1 (",  explained_var[1], "%)", sep="")) + ylab(paste("PC2 (",  explained_var[2], "%)", sep=""))+ ggtitle(paste("PCA by Assigned POP", sep = ""))

# Save the PCA plots as JPEG files
plots <- list(plot1, plot2)
for (i in seq_along(plots)) {
  ggsave(filename = file.path(PCA_dir, paste0("pca_plot_", i, ".jpeg")), plot = plots[[i]], width = 8, height = 6, dpi = 600)
}

```

```{r #4.d DB European}
# Removal on non-European ancestry individuals
system("plink --bfile 2_QC/QC9_hwe --keep 3_PCA/closestPOP_EUR.txt --make-bed --out 2_QC/QC12_EUR")

# PCA calculation (for future statistical analysis)
system("plink --bfile 2_QC/QC12_EUR --extract 3_PCA/SNP_to_PCA.prune.in --pca var-wts --out 6_Statistical_analysis/PCA_analysis")

# Save the environment for backup
save.image(paste(main, "Env_chunk4.RData", sep = ""))

print("End of chunk 4")
# end of chunk 4
```

```{r #5.a Imputation}
# Extraction and conversion of chromosomes to vcf format, which is the required input format for the Michigan Imputation Server
for (i in 1:23) {
  cmd = sprintf("plink --bfile 2_QC/QC12_EUR --recode vcf bgz --snps-only just-acgt --chr %d --out 4_Imputation/Pre_imputation_chr%d", i, i)
  system(cmd)
}


# Save the environment for backup
save.image(paste(main, "Env_chunk5.RData", sep = ""))

print("End of chunk 5a")
# end of chunk 5a
```

```{r #5.b Imputation - Possible issues with Michigan Imputation Server}
# It is common that the first imputation attempt fails because the Michigan Imputation server identifies SNPs that have inverted reference and alternative alleles between your data and reference panel data, or the alleles are reported according to the opposite strand. Sometimes, there may be allele mismatches as well. In such cases, the Michigan Imputation server produces a file "snps-excluded.txt" containing all relevant information. 
# The following code is aimed at solving issues of such a kind.

if (file.exists(paste(Imputation_dir,"snps-excluded.txt", sep=""))) {
# Upload the files for flipping or removal
snps.excluded = read.delim(paste(Imputation_dir,"snps-excluded.txt", sep=""), header=TRUE)
names(snps.excluded)[1] = "SNP"
summary(as.factor(snps.excluded$INFO))
# Select the SNPs to remove
snps_to_exclude = snps.excluded[grepl("Allele mismatch|Monomorphic", snps.excluded$INFO), ]
# Select the SNPs for alleles switching
snps_to_switch = snps.excluded[grepl("Allele switch", snps.excluded$INFO), ]
snps_to_switch$REF_new = snps_to_switch$ALT
snps_to_switch$ALT_new = snps_to_switch$REF
snps_to_switch = select(snps_to_switch, c("SNP", "REF", "ALT", "REF_new", "ALT_new"))
# Select the SNPs for alleles flipping
snps_to_flip = snps.excluded[grepl("Strand flip", snps.excluded$INFO), ]


# Save the column as a text file without a header
write.table(snps_to_flip$SNP, file = paste(Imputation_dir, "snps_to_flip.txt", sep=""), quote = FALSE, row.names = FALSE, col.names = FALSE)
write.table(snps_to_exclude$SNP, file = paste(Imputation_dir, "snps_to_exclude.txt", sep=""), quote = FALSE, row.names = FALSE, col.names = FALSE)
write.table(snps_to_switch, file = paste(Imputation_dir, "snps_to_switch.txt", sep=""), quote = FALSE, row.names = FALSE, col.names = FALSE)

# Flip or remove the SNPs
system("plink --bfile 2_QC/QC12_EUR --exclude 4_Imputation/snps_to_exclude.txt --update-alleles 4_Imputation/snps_to_switch.txt --flip 4_Imputation/snps_to_flip.txt --make-bed  --out 4_Imputation/QC13_EUR_flipped")

# Repeat the vcf coding
for (i in 1:23) {
  cmd = sprintf("plink --bfile 4_Imputation/QC13_EUR_flipped --recode vcf bgz --snps-only just-acgt --chr %d --out 4_Imputation/Pre_imputation_flipped_chr%d", i, i)
  system(cmd)
}
} # end of if condition (line 385)

# Save the environment for backup
save.image(paste(main, "Env_chunk5.RData", sep = ""))

print("End of chunk 5b")
# end of chunk 5b
```

```{r #6 Post-imputation quality controls}
# For time and space reasons, we will work only on the chromosome 22.
# The Michigan Imputation Server sends a link and a password for the download of each chromosome separately.
# The procedure is reported below for chr22 only:

# unzip the zipped file of chromosome 22, which should be downloaded into the folder 5_PostImputation
system("7z e chr_22.zip")

# extract the .info file, which contains the quality of imputation for each SNP
system("gzip -d chr22.info.gz") 

# apply a quality score filter based on an info-score value>0.7
system("awk '$7 >= 0.7' chr22.info > info_score_0.7.txt") 

# create a file containing only the list of SNPs to extract based on the previous quality filter 
system("cut -f1 info_score_0.7.txt > to_extract_info_score_0.7.txt") 

# create a new dataset in binary plink format extracting only the high-quality imputed SNPs
system("plink --vcf chr22.dose.vcf.gz --extract to_extract_info_score_0.7.txt --make-bed --out chr22_info_score_0.7") 
# apply a filter based on genotyping call rate
system("plink --bfile 5_PostImputation/chr22_info_score_0.7 --geno 0.10 --make-bed --out 5_PostImputation/chr22_info_score_0.7_geno") 

# apply a filter based on subject call rate
system("plink --bfile 5_PostImputation/chr22_info_score_0.7_geno --mind 0.02 --make-bed --out 5_PostImputation/chr22_info_score_0.7_geno_mind") 

# Now imagine to repeat the process for each chromosome and merge them all together using the following command to create a final unique imputed database:
# system("plink --bfile chr1_info_score_0.7_geno_mind --merge-list 5_PostImputation/merge_file.txt --make-bed --out 5_PostImputation/Imputed_DB") 
# the merge_list.txt file is created directly by you, creating three columns reporting the name of each chromosome plus the extensions .bed, .bim, and .fam, respectively
# Specifically, the format is directory/file1.bed directory/file1.bim directory/file1.fam
#                             directory/file2.bed directory/file2.bim directory/file2.fam


# Save the environment for backup
save.image(paste(main, "Env_chunk6.RData", sep = ""))

print("End of chunk 6")
# end of chunk 

```

```{r #7.a Statistical analysis - no adjustment}
# We will use the non-imputed database for statistical analysis. The difference with the imputed one is only dependent on the time and space required to complete the process.

# To run the logistic regression we will use the --logistic function, without adjusting for covariates. Base adjustment only include sex
system("plink --bfile 2_QC/QC12_EUR --logistic --ci 0.95 --sex --freq --hide-covar --out 6_Statistical_analysis/Unadjusted_LR")

# This command calculates the frequencies in cases and controls of the genetic variants under analysis
system("plink --bfile 2_QC/QC12_EUR --freq case-control --out 6_Statistical_analysis/Unadjusted_LR_freq")


Unadjusted_LR_assoc <- read.xxx(paste(Analysis_dir/Unadjusted_LR.assoc, sep=""))
Unadjusted_LR_freq <- read.xxx(paste(Analysis_dir/Unadjusted_LR.freq, sep=""))
# merge the results file
results_unadjusted <- merge(Unadjusted_LR.assoc, Unadjusted_LR.freq)
# order by p-value
results_unadjusted <- results_unadjusted %>% arrange(P)

check altro

# create a Manhattan plot
png(paste(Analysis_dir/"Unadjusted_Manhattan_plot.png", sep=""), height = 2000, width = 3000, units = "px", res = 600, )
print(manhattan(Unadjusted_LR_assoc, annotatePval =
5e-05,chr="CHR", bp="BP", snp="SNP",
p="P", ylim = c(0, 13),chrlabs = c(1:23), suggestiveline =
-log10(1e-05), genomewideline = -log10(5e-08)))
dev.off()

# create a qqplot
png(paste(Analysis_dir/"Unadjusted_qqplot.png", sep=""), width = 2000, height = 2000, units="px", res=400, type="cairo")
lambda <- median(qchisq(1-na.omit(Unadjusted_LR_assoc$P),1))/qchisq(0.5,1)
qq(Unadjusted_LR_assoc$P)
text(1,6, paste("lambda","=",  signif(lambda, digits = 3)) )
dev.off()

# Save the results with p-value < 0.05 into a separate file
Results5e5 <- subset(Unadjusted_LR_assoc, (Unadjusted_LR_assoc$P < 0.00005))
Results5e5 <- merge(Results5e5,Freq)
Results5e5$NCHROBS = NULL
write.table(Results5e5, paste(Analysis_dir, "Unadjusted_results5e5.txt", sep=""),
row.names=F, col.names=T, sep="\t", quote=F)


print("End of chunk 7.a")
# end of chunk 7.a
```

```{r #7.b Statistical analysis - with adjustment}


print("End of chunk XXXX")
# end of chunk 7.b
```

```{r #7.c Results comparison}


print("End of chunk 6.c")
# end of chunk 7.c
```



